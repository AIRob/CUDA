{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Stream의 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sgemm_stream.cu\n"
     ]
    }
   ],
   "source": [
    "%%file sgemm_stream.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "typedef enum TARGET {HOST, DEVICE} TARGET;\n",
    "typedef enum MEMTYPE {NORMAL, PINNED} MEMTYPE;\n",
    "\n",
    "typedef struct {\n",
    "    int width;\n",
    "    int height;\n",
    "    float *elements;\n",
    "} Matrix;\n",
    "\n",
    "__global__ void sgemm(Matrix A, Matrix B, Matrix C, \n",
    "                      const float alpha, const float beta, \n",
    "                      const int width, const int height) {\n",
    "    int idx_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int idx_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int idx = idx_y * width + idx_x;\n",
    "    \n",
    "    if (idx_x >= width || idx_y >= height)\n",
    "        return;\n",
    "    \n",
    "    float value = 0.f;\n",
    "    for (int e = 0; e < width; e++)\n",
    "        value += A.elements[idx_y * width + e] * B.elements[e * width + idx_x];\n",
    "    C.elements[idx] = alpha * value + beta * C.elements[idx];\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target = HOST, MEMTYPE memtype = NORMAL);\n",
    "\n",
    "int main(int argv, char* argc[]) {\n",
    "    Matrix A, B, C, C_dst;\n",
    "    Matrix dA, dB, dC[2];\n",
    "    const float alpha = 2.f;\n",
    "    const float beta = .5f;\n",
    "    const int width = 2048;\n",
    "    const int height = 2048;\n",
    "    float elapsed_gpu;\n",
    "    double elapsed_cpu;\n",
    "    \n",
    "    // Select Host memory type (NORMAL, PINNED)\n",
    "    MEMTYPE memtype = PINNED;\n",
    "    \n",
    "    // CUDA Event Create to estimate elased time\n",
    "    cudaEvent_t start, stop;\n",
    "    struct timespec begin, finish;\n",
    "    \n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Create CUDA Stream\n",
    "    cudaStream_t stream[2];\n",
    "    // TODO: Write CUDA Stream creation code\n",
    "\n",
    "    /////////////\n",
    "    \n",
    "    // Initialize host matrix\n",
    "    InitMatrix(A, width, height, HOST, memtype);\n",
    "    InitMatrix(B, width, height, HOST, memtype);\n",
    "    InitMatrix(C, width, height, HOST, memtype);\n",
    "    InitMatrix(C_dst, width, height, HOST, memtype);\n",
    "\n",
    "    // CUDA Memory Initialize\n",
    "    InitMatrix(dA, width, height, DEVICE);\n",
    "    InitMatrix(dB, width, height, DEVICE);\n",
    "    InitMatrix(dC[0], width, height, DEVICE);\n",
    "    InitMatrix(dC[1], width, height, DEVICE);\n",
    "    \n",
    "    // CUDA Operation\n",
    "    cudaEventRecord(start, 0);\n",
    "    clock_gettime(CLOCK_MONOTONIC, &begin);\n",
    "    \n",
    "    // Copy host data to the device (CUDA global memory)\n",
    "    cudaMemcpyAsync(dA.elements, A.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpyAsync(dB.elements, B.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    \n",
    "    // Launch GPU Kernel\n",
    "    dim3 blockDim(16, 16);\n",
    "    dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        // TODO: Write CUDA Stream enabled operation\n",
    "        cudaMemcpyAsync(dC[0].elements, C.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "        \n",
    "        sgemm<<<gridDim, blockDim>>>(dA, dB, dC[0], alpha, beta, width, height);\n",
    "        \n",
    "        cudaMemcpyAsync(C_dst.elements, dC[0].elements, width * height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "        //////////////\n",
    "    }\n",
    "        \n",
    "    clock_gettime(CLOCK_MONOTONIC, &finish);\n",
    "    cudaEventRecord(stop, 0);\n",
    "    \n",
    "    // Estimate CUDA operation time\n",
    "    cudaEventRecord(stop, 0);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    cudaEventElapsedTime(&elapsed_gpu, start, stop);\n",
    "    printf(\"SGEMM CUDA Elapsed time: %f ms\\n\", elapsed_gpu);\n",
    "    elapsed_cpu = (finish.tv_sec - begin.tv_sec);\n",
    "    elapsed_cpu += (finish.tv_nsec - begin.tv_nsec) / 1000000000.0;\n",
    "    printf(\"Host time: %f ms\\n\", elapsed_cpu * 1000);\n",
    "    \n",
    "    // finalize CUDA stream\n",
    "    // TODO: Write CUDA Stream destory code\n",
    "    \n",
    "    /////////////////\n",
    "    \n",
    "    // finalize CUDA event\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    // Finalize\n",
    "    cudaFree(dA.elements);\n",
    "    cudaFree(dB.elements);\n",
    "    cudaFree(dC[0].elements);\n",
    "    cudaFree(dC[1].elements);\n",
    "    \n",
    "    if (memtype == NORMAL) {\n",
    "        free(A.elements);\n",
    "        free(B.elements);\n",
    "        free(C.elements);\n",
    "    }\n",
    "    else {\n",
    "        cudaFreeHost(A.elements);\n",
    "        cudaFreeHost(B.elements);\n",
    "        cudaFreeHost(C.elements);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target, MEMTYPE memtype) {\n",
    "    mat.width = width;\n",
    "    mat.height = height;\n",
    "    \n",
    "    if (target == DEVICE) {\n",
    "        cudaMalloc((void**)&mat.elements, width * height * sizeof(float));\n",
    "    }\n",
    "    else {\n",
    "        if (memtype == NORMAL)\n",
    "            mat.elements = (float*)malloc(width * height * sizeof(float));\n",
    "        else\n",
    "            cudaHostAlloc(&mat.elements, width * height * sizeof(float), cudaHostAllocDefault);\n",
    "    \n",
    "        for (int row = 0; row < height; row++) {\n",
    "            for (int col = 0; col < width; col++) {\n",
    "                mat.elements[row * width + col] = row * width + col * 0.001;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc sgemm_stream.cu --ptxas-options=--verbose -gencode=arch=compute_35,code=sm_35 -I/usr/local/cuda/samples/common/inc -o sgemm_stream\n",
      "sgemm_stream.cu(54): warning: variable \"stream\" was declared but never referenced\n",
      "\n",
      "sgemm_stream.cu(54): warning: variable \"stream\" was declared but never referenced\n",
      "\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z5sgemm6MatrixS_S_ffii' for 'sm_35'\n",
      "ptxas info    : Function properties for _Z5sgemm6MatrixS_S_ffii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 13 registers, 384 bytes cmem[0]\n"
     ]
    }
   ],
   "source": [
    "! make sgemm_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGEMM CUDA Elapsed time: 278.760559 ms\r\n",
      "Host time: 0.077756 ms\r\n"
     ]
    }
   ],
   "source": [
    "! ./sgemm_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sgemm_stream_example.cu\n"
     ]
    }
   ],
   "source": [
    "%%file sgemm_stream_example.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "typedef enum TARGET {HOST, DEVICE} TARGET;\n",
    "typedef enum MEMTYPE {NORMAL, PINNED} MEMTYPE;\n",
    "\n",
    "typedef struct {\n",
    "    int width;\n",
    "    int height;\n",
    "    float *elements;\n",
    "} Matrix;\n",
    "\n",
    "__global__ void sgemm(Matrix A, Matrix B, Matrix C, \n",
    "                      const float alpha, const float beta, \n",
    "                      const int width, const int height) {\n",
    "    int idx_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int idx_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int idx = idx_y * width + idx_x;\n",
    "    \n",
    "    if (idx_x >= width || idx_y >= height)\n",
    "        return;\n",
    "    \n",
    "    float value = 0.f;\n",
    "    for (int e = 0; e < width; e++)\n",
    "        value += A.elements[idx_y * width + e] * B.elements[e * width + idx_x];\n",
    "    C.elements[idx] = alpha * value + beta * C.elements[idx];\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target = HOST, MEMTYPE memtype = NORMAL);\n",
    "\n",
    "int main(int argv, char* argc[]) {\n",
    "    Matrix A[2], B[2], C[2], C_src;\n",
    "    Matrix dA[2], dB[2], dC[2];\n",
    "    const float alpha = 2.f;\n",
    "    const float beta = .5f;\n",
    "    const int width = 2048;\n",
    "    const int height = 2048;\n",
    "    float elapsed_gpu;\n",
    "    double elapsed_cpu;\n",
    "    \n",
    "    // Select Host memory type (NORMAL, PINNED)\n",
    "    MEMTYPE memtype = PINNED;\n",
    "    \n",
    "    // CUDA Event Create to estimate elased time\n",
    "    cudaEvent_t start, stop;\n",
    "    struct timespec begin, finish;\n",
    "    \n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Initialize host matrix\n",
    "    for (int i = 0; i < 2; i++) {\n",
    "        InitMatrix(A[i], width, height, HOST, memtype);\n",
    "        InitMatrix(B[i], width, height, HOST, memtype);\n",
    "        InitMatrix(C[i], width, height, HOST, memtype);\n",
    "    }\n",
    "    InitMatrix(C_src, width, height, HOST, memtype);\n",
    "    \n",
    "    // CUDA Memory Initialize\n",
    "    for (int i = 0; i < 2; i++) {\n",
    "        InitMatrix(dA[i], width, height, DEVICE);\n",
    "        InitMatrix(dB[i], width, height, DEVICE);\n",
    "        InitMatrix(dC[i], width, height, DEVICE);\n",
    "    }\n",
    "    \n",
    "    // Create CUDA Stream\n",
    "    cudaStream_t stream[2];\n",
    "    cudaStreamCreate(&stream[0]);\n",
    "    cudaStreamCreate(&stream[1]);\n",
    "    \n",
    "    // CUDA Operation\n",
    "    cudaEventRecord(start, 0);\n",
    "    clock_gettime(CLOCK_MONOTONIC, &begin);\n",
    "    \n",
    "    int idx_stream = 0;\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        // Copy host data to the device (CUDA global memory)\n",
    "        cudaMemcpyAsync(dA[idx_stream].elements, A[idx_stream].elements, width * height * sizeof(float), cudaMemcpyHostToDevice, stream[idx_stream]);\n",
    "        cudaMemcpyAsync(dB[idx_stream].elements, B[idx_stream].elements, width * height * sizeof(float), cudaMemcpyHostToDevice, stream[idx_stream]);\n",
    "        cudaMemcpyAsync(dC[idx_stream].elements, C_src.elements, width * height * sizeof(float), cudaMemcpyHostToDevice, stream[idx_stream]);\n",
    "    \n",
    "        // Launch GPU Kernel\n",
    "        dim3 blockDim(16, 16);\n",
    "        dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n",
    "        sgemm<<<gridDim, blockDim, 0, stream[idx_stream]>>>(dA[idx_stream], dB[idx_stream], dC[idx_stream], alpha, beta, width, height);\n",
    "    \n",
    "        // Copy computation result from the Device the host memory\n",
    "        cudaMemcpyAsync(C[idx_stream].elements, dC[idx_stream].elements, width * height * sizeof(float), cudaMemcpyDeviceToHost, stream[idx_stream]);\n",
    "        \n",
    "        idx_stream = (idx_stream == 0) ? 1 : 0;\n",
    "    }\n",
    "    clock_gettime(CLOCK_MONOTONIC, &finish);\n",
    "    cudaEventRecord(stop, 0);\n",
    "    \n",
    "    // Estimate CUDA operation time\n",
    "    cudaEventRecord(stop, 0);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    cudaEventElapsedTime(&elapsed_gpu, start, stop);\n",
    "    printf(\"SGEMM CUDA Elapsed time: %f ms\\n\", elapsed_gpu);\n",
    "    elapsed_cpu = (finish.tv_sec - begin.tv_sec);\n",
    "    elapsed_cpu += (finish.tv_nsec - begin.tv_nsec) / 1000000000.0;\n",
    "    printf(\"Host time: %f ms\\n\", elapsed_cpu * 1000);\n",
    "    \n",
    "    // finalize CUDA stream\n",
    "    cudaStreamDestroy(stream[0]);\n",
    "    cudaStreamDestroy(stream[1]);\n",
    "    \n",
    "    // finalize CUDA event\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    // Finalize\n",
    "    for (int i = 0; i < 2; i++) {\n",
    "        cudaFree(dA[i].elements);\n",
    "        cudaFree(dB[i].elements);\n",
    "        cudaFree(dC[i].elements);\n",
    "    \n",
    "    \n",
    "        if (memtype == NORMAL) {\n",
    "            free(A[i].elements);\n",
    "            free(B[i].elements);\n",
    "            free(C[i].elements);\n",
    "        }\n",
    "        else {\n",
    "            cudaFreeHost(A[i].elements);\n",
    "            cudaFreeHost(B[i].elements);\n",
    "            cudaFreeHost(C[i].elements);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target, MEMTYPE memtype) {\n",
    "    mat.width = width;\n",
    "    mat.height = height;\n",
    "    \n",
    "    if (target == DEVICE) {\n",
    "        cudaMalloc((void**)&mat.elements, width * height * sizeof(float));\n",
    "    }\n",
    "    else {\n",
    "        if (memtype == NORMAL)\n",
    "            mat.elements = (float*)malloc(width * height * sizeof(float));\n",
    "        else\n",
    "            cudaHostAlloc(&mat.elements, width * height * sizeof(float), cudaHostAllocDefault);\n",
    "    \n",
    "        for (int row = 0; row < height; row++) {\n",
    "            for (int col = 0; col < width; col++) {\n",
    "                mat.elements[row * width + col] = row * width + col * 0.001;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
