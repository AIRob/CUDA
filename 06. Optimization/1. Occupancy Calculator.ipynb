{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Optimization\n",
    "\n",
    "이번 Tutorial에서는 CUDA 최적화에 대하여 다뤄보도록 하겠습니다.\n",
    "\n",
    "CUDA 프로그래밍에 대한 최적화 기법은 다음과 같이 정리할 수 있습니다.\n",
    "* 병렬처리 최적화\n",
    "    * CUDA Occupancy 최적화\n",
    "    * Asynchronous Operation\n",
    "* 대역폭 최적화\n",
    "    * CPU  GPU 데이터 전송 최적화\n",
    "    * GPU 메모리 최적화\n",
    "* 분기 최적화\n",
    "* 명령어 최적화\n",
    "\n",
    "하나씩 살펴 보면서, 성능이 어떻게 향상되어 가는지 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SGEMM Example & Occupancy Calculator\n",
    "\n",
    "최적화를 익히기 위한 예제인 SGEMM을 살펴보도록 하겠습니다.\n",
    "\n",
    "SGEMM은 다음과 같이 Matrix Multiplication + Summation을 말합니다.\n",
    "$$ c_{ij} = \\alpha \\sum _{k=1} ^m a_{ik} b_{kj} + \\beta c_{ij} \\space\\space for \\space\\space C \\leftarrow \\alpha A B + \\beta C$$\n",
    "\n",
    "이 코드에는 sgemm 코드와 함께 수행시간을 측정하기 위한 코드가 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sgemm.cu\n"
     ]
    }
   ],
   "source": [
    "%%file sgemm.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "typedef enum TARGET {HOST, DEVICE} TARGET;\n",
    "\n",
    "typedef struct {\n",
    "    int width;\n",
    "    int height;\n",
    "    float *elements;\n",
    "} Matrix;\n",
    "\n",
    "__global__ void sgemm(Matrix A, Matrix B, Matrix C, \n",
    "                      const float alpha, const float beta, \n",
    "                      const int width, const int height) {\n",
    "    int idx_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int idx_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    \n",
    "    if (idx_x >= width || idx_y >= height)\n",
    "        return;\n",
    "    \n",
    "    // TODO: Write sgemm code \"C = alpha * A * B + beta * C\"\n",
    "    float value = 0.f;\n",
    "    for (int i = 0; i < width; i++)\n",
    "        value += value + A.elements[idx_y * width + i] * B.elements[i * width + idx_x];\n",
    "    C.elements[idx_y * width + idx_x] = alpha * value + beta * C.elements[idx_y * width + idx_x];\n",
    "    printf(\"%f (%d, %d)\\n\", alpha * value + beta * C.elements[idx_y * width + idx_x], idx_y, idx_x);\n",
    "    /////////////\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target = HOST);\n",
    "bool IsMatDiff(Matrix &A, Matrix &B);\n",
    "void sgemm_host(Matrix &A, Matrix &B, Matrix &C,\n",
    "               const float alpha, const float beta,\n",
    "               const int width, const int height);\n",
    "\n",
    "int main(int argv, char* argc[]) {\n",
    "    Matrix A, B, C_host, C_device;\n",
    "    Matrix dA, dB, dC;\n",
    "    const float alpha = 2.f;\n",
    "    const float beta = .5f;\n",
    "    const int width = 4;\n",
    "    const int height = width;\n",
    "    float elapsed_gpu;\n",
    "    double elapsed_cpu;\n",
    "    \n",
    "    // CUDA Event Create to estimate elased time\n",
    "    cudaEvent_t start, stop;\n",
    "    struct timespec begin, finish;\n",
    "    \n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Initialize host matrix\n",
    "    InitMatrix(A, width, height);\n",
    "    InitMatrix(B, width, height);\n",
    "    InitMatrix(C_host, width, height);\n",
    "    InitMatrix(C_device, width, height);\n",
    "\n",
    "    // CUDA Memory Initialize\n",
    "    InitMatrix(dA, width, height, DEVICE);\n",
    "    InitMatrix(dB, width, height, DEVICE);\n",
    "    InitMatrix(dC, width, height, DEVICE);\n",
    "    \n",
    "    // CUDA Operation\n",
    "    cudaEventRecord(start, 0);\n",
    "    clock_gettime(CLOCK_MONOTONIC, &begin);\n",
    "    \n",
    "    //////////////////////\n",
    "    // Copy host data to the device (CUDA global memory)\n",
    "    // TODO: Write CUDA Memcpy code (cpu -> gpu)\n",
    "    cudaMemcpy(dA.elements, A.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(dB.elements, B.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(dC.elements, C_device.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch GPU Kernel\n",
    "    // TODO: Write sgemm Kernel Execution Code\n",
    "    // Please refer kernel code above.\n",
    "    dim3 blockDim(16, 16);\n",
    "    dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n",
    "    sgemm<<<gridDim, blockDim>>>(dA, dB, dC, alpha, beta, width, height);\n",
    "    \n",
    "    // Copy computation result from the Device the host memory\n",
    "    // TODO: Write CUDA memcpy code (gpu -> cpu)\n",
    "    cudaMemcpy(C_device.elements, dC.elements, width * height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    //////////////////////\n",
    "    clock_gettime(CLOCK_MONOTONIC, &finish);\n",
    "    cudaEventRecord(stop, 0);\n",
    "    \n",
    "    // Estimate CUDA operation time\n",
    "    cudaEventRecord(stop, 0);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    cudaEventElapsedTime(&elapsed_gpu, start, stop);\n",
    "    printf(\"SGEMM CUDA Elapsed time: %f ms\\n\", elapsed_gpu);\n",
    "    elapsed_cpu = (finish.tv_sec - begin.tv_sec);\n",
    "    elapsed_cpu += (finish.tv_nsec - begin.tv_nsec) / 1000000000.0;\n",
    "    printf(\"Host time: %f ms\\n\", elapsed_cpu * 1000);\n",
    "    \n",
    "    // Compute CPU Operation\n",
    "    clock_gettime(CLOCK_MONOTONIC, &begin);\n",
    "    sgemm_host(A, B, C_host, alpha, beta, width, height);\n",
    "    clock_gettime(CLOCK_MONOTONIC, &finish);\n",
    "    \n",
    "    elapsed_cpu = (finish.tv_sec - begin.tv_sec);\n",
    "    elapsed_cpu += (finish.tv_nsec - begin.tv_nsec) / 1000000000.0;\n",
    "    printf(\"SGEMM CPU only time: %f ms\\n\", elapsed_cpu * 1000);\n",
    "    \n",
    "    if (IsMatDiff(C_host, C_device)) {\n",
    "        printf(\"Something wrong!!\\n\");\n",
    "    }\n",
    "    else {\n",
    "        printf(\"Success !!\\n\");\n",
    "    }\n",
    "    \n",
    "    // finalize CUDA event\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    // Finalize\n",
    "    cudaFree(dA.elements);\n",
    "    cudaFree(dB.elements);\n",
    "    cudaFree(dC.elements);\n",
    "    \n",
    "    free(A.elements);\n",
    "    free(B.elements);\n",
    "    free(C_host.elements);\n",
    "    free(C_device.elements);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target) {\n",
    "    mat.width = width;\n",
    "    mat.height = height;\n",
    "    \n",
    "    if (target == DEVICE) {\n",
    "        cudaMalloc((void**)&mat.elements, width * height * sizeof(float));\n",
    "    }\n",
    "    else {\n",
    "        mat.elements = (float*)malloc(width * height * sizeof(float));\n",
    "    \n",
    "        for (int row = 0; row < height; row++) {\n",
    "            for (int col = 0; col < width; col++) {\n",
    "                mat.elements[row * width + col] = row * width + col * 0.001;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bool IsMatDiff(Matrix &A, Matrix &B) {\n",
    "    if (A.width != B.width || A.height != B.height) {\n",
    "        return true;\n",
    "    }\n",
    "    \n",
    "    unsigned int count = 0;\n",
    "    for (int row = 0; row < A.height; row++) {\n",
    "        for (int col = 0; col < A.width; col++) {\n",
    "            //count += (A.elements[row * A.width + col] - B.elements[row * A.width + col]) * \\\n",
    "            //    (A.elements[row * A.width + col] - B.elements[row * A.width + col]);\n",
    "            count += A.elements[row * A.width + col] != B.elements[row * A.width + col] ? 1 : 0;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"%d\\n\", count);\n",
    "    if (count != 0.f) {\n",
    "        \n",
    "        return true;\n",
    "    }\n",
    "    return false;\n",
    "}\n",
    "\n",
    "void sgemm_host(Matrix &A, Matrix &B, Matrix &C, const float alpha, const float beta, const int width, const int height) {\n",
    "    for (int row = 0; row < C.height; row++) {\n",
    "        for (int col = 0; col < C.width; col++) {\n",
    "            float value = 0.f;\n",
    "            for (int e = 0; e < C.width; e++)\n",
    "                value += A.elements[row * width + e] * B.elements[e * width + col];\n",
    "            C.elements[row * width + col] = alpha * value + beta * C.elements[row * width + col];\n",
    "            printf(\"%f (%d, %d)\\n\", alpha * value + beta * C.elements[row * width + col], row, col);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc sgemm.cu -gencode=arch=compute_30,code=sm_30 -o sgemm \r\n"
     ]
    }
   ],
   "source": [
    "! make sgemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.168000 (0, 0)\r\n",
      "0.168268 (0, 1)\r\n",
      "0.168536 (0, 2)\r\n",
      "0.168804 (0, 3)\r\n",
      "289.167969 (1, 0)\r\n",
      "289.216278 (1, 1)\r\n",
      "289.264526 (1, 2)\r\n",
      "289.312775 (1, 3)\r\n",
      "578.167969 (2, 0)\r\n",
      "578.264282 (2, 1)\r\n",
      "578.360535 (2, 2)\r\n",
      "578.456787 (2, 3)\r\n",
      "867.167969 (3, 0)\r\n",
      "867.312256 (3, 1)\r\n",
      "867.456543 (3, 2)\r\n",
      "867.600830 (3, 3)\r\n",
      "SGEMM CUDA Elapsed time: 1.060288 ms\r\n",
      "Host time: 1.308397 ms\r\n",
      "0.168000 (0, 0)\r\n",
      "0.168268 (0, 1)\r\n",
      "0.168536 (0, 2)\r\n",
      "0.168804 (0, 3)\r\n",
      "289.167969 (1, 0)\r\n",
      "289.216278 (1, 1)\r\n",
      "289.264526 (1, 2)\r\n",
      "289.312805 (1, 3)\r\n",
      "578.167969 (2, 0)\r\n",
      "578.264282 (2, 1)\r\n",
      "578.360596 (2, 2)\r\n",
      "578.456787 (2, 3)\r\n",
      "867.168091 (3, 0)\r\n",
      "867.312256 (3, 1)\r\n",
      "867.456604 (3, 2)\r\n",
      "867.600830 (3, 3)\r\n",
      "SGEMM CPU only time: 0.029641 ms\r\n",
      "4\r\n",
      "Something wrong!!\r\n"
     ]
    }
   ],
   "source": [
    "! ./sgemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Makefile\n"
     ]
    }
   ],
   "source": [
    "%%file Makefile\n",
    "\n",
    "NVCC = nvcc\n",
    "NVCC_OPTS = --ptxas-options=--verbose -gencode=arch=compute_35,code=sm_35 -I/usr/local/cuda/samples/common/inc\n",
    "\n",
    "all: sgemm\n",
    "    \n",
    "sgemm: sgemm.cu\n",
    "\t$(NVCC) sgemm.cu -gencode=arch=compute_30,code=sm_30 -o sgemm \n",
    "    \n",
    "sgemm_v: \n",
    "\t$(NVCC) sgemm.cu $(NVCC_OPTS) -o sgemm \n",
    "    \n",
    "sgemm_resize_block: sgemm_resize_block.cu\n",
    "\t$(NVCC) sgemm_resize_block.cu $(NVCC_OPTS) -o sgemm_resize_block\n",
    "    \n",
    "sgemm_async_copy: sgemm_async_copy.cu\n",
    "\t$(NVCC) sgemm_async_copy.cu $(NVCC_OPTS) -o sgemm_async_copy\n",
    "    \n",
    "sgemm_pinned_memory: sgemm_pinned_memory.cu\n",
    "\t$(NVCC) sgemm_pinned_memory.cu $(NVCC_OPTS) -o sgemm_pinned_memory\n",
    "\n",
    "sgemm_stream: sgemm_stream.cu\n",
    "\t$(NVCC) sgemm_stream.cu $(NVCC_OPTS) -o sgemm_stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Event\n",
    "CUDA Kernel의 전체 수행시간을 측정하기 위해서 CUDA Event를 사용했습니다.\n",
    "CUDA Event는 CPU programming에서 사용하는 Event handle과 같이 다양하게 활용할 수 있습니다. 하지만 이번 시간에는 CUDA의 성능향상을 확인하기 위한 용도로서 CUDA Kernel의 수행시간을 측정하는 부분에만 집중하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cudaEvent.hold\n"
     ]
    }
   ],
   "source": [
    "%%file cudaEvent.hold\n",
    "// CUDA Event Create to estimate elased time\n",
    "cudaEvent_t start, stop;\n",
    "cudaeventRecord(start, 0);\n",
    "cudaEventRecord(stop, 0);\n",
    "cudaEventSynchronize(stop);\n",
    "\n",
    "cudaEventElapsedTime(&elapsed_gpu, start, stop);\n",
    "printf(\"SGEMM CUDA Elapsed time: %f ms\\n\", elapsed_gpu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Occupancy Calculator\n",
    "\n",
    "![](./CUDA Occupancy Calculator.png)\n",
    "CUDA Occupancy Calculator는 CUDA SM 상에서 자원을 효율적으로 활용하기 위해 필요한 이론적 계산치를 계산하는데 유용한 엑셀파일입니다. [다운로드](./CUDA_Occupancy_calculator.xls)\n",
    "\n",
    "여기에 입력할 정보는 대부분은 CUDA 코드를 작성하시면서 설계하는 값이지만, CUDA thread당 register의 수는 compiler에서 사용한 결과를 확인할 수 밖에 없습니다. 따라서 nvcc에게 다음과 같은 option을 주어서 CUDA thread 당 register 수를 확인할 수 있습니다.\n",
    "\n",
    "**--ptxas-options=--verbose**\n",
    "\n",
    "Compile 옵션을 주고 나온 결과를 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc sgemm.cu --ptxas-options=--verbose -gencode=arch=compute_35,code=sm_35 -o sgemm \n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z5sgemm6MatrixS_S_ffii' for 'sm_35'\n",
      "ptxas info    : Function properties for _Z5sgemm6MatrixS_S_ffii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 13 registers, 384 bytes cmem[0]\n"
     ]
    }
   ],
   "source": [
    "! make sgemm_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 이 값을 가지고 Occupancy Calculator를 통해 어느정도 최적화 되어있는지 확인해봅시다.\n",
    "그리고 다른 크기로 바꾸어서 성능이 어떻게 바뀌는지 확인해보세요. CPU 코드는 느려서 지웠습니다. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sgemm_resize_block.cu\n"
     ]
    }
   ],
   "source": [
    "%%file sgemm_resize_block.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "typedef enum TARGET {HOST, DEVICE} TARGET;\n",
    "\n",
    "typedef struct {\n",
    "    int width;\n",
    "    int height;\n",
    "    float *elements;\n",
    "} Matrix;\n",
    "\n",
    "__global__ void sgemm(Matrix A, Matrix B, Matrix C, \n",
    "                      const float alpha, const float beta, \n",
    "                      const int width, const int height) {\n",
    "    int idx_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int idx_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int idx = idx_y * width + idx_x;\n",
    "    \n",
    "    if (idx_x >= width || idx_y >= height)\n",
    "        return;\n",
    "    \n",
    "    // TODO: Copy sgemm code from above you write\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target = HOST);\n",
    "\n",
    "int main(int argv, char* argc[]) {\n",
    "    Matrix A, B, C_host, C_device;\n",
    "    Matrix dA, dB, dC;\n",
    "    const float alpha = 2.f;\n",
    "    const float beta = .5f;\n",
    "    const int width = 2048;\n",
    "    const int height = 2048;\n",
    "    float elapsed_gpu;\n",
    "    double elapsed_cpu;\n",
    "    \n",
    "    // CUDA Event Create to estimate elased time\n",
    "    cudaEvent_t start, stop;\n",
    "    struct timespec begin, finish;\n",
    "    \n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Initialize host matrix\n",
    "    InitMatrix(A, width, height);\n",
    "    InitMatrix(B, width, height);\n",
    "    InitMatrix(C_device, width, height);\n",
    "\n",
    "    // CUDA Memory Initialize\n",
    "    InitMatrix(dA, width, height, DEVICE);\n",
    "    InitMatrix(dB, width, height, DEVICE);\n",
    "    InitMatrix(dC, width, height, DEVICE);\n",
    "    \n",
    "    // CUDA Operation\n",
    "    cudaEventRecord(start, 0);\n",
    "    clock_gettime(CLOCK_MONOTONIC, &begin);\n",
    "    \n",
    "    // Copy host data to the device (CUDA global memory)\n",
    "    cudaMemcpy(dA.elements, A.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(dB.elements, B.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(dC.elements, C_device.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch GPU Kernel\n",
    "    // TODO: Defic\n",
    "    \n",
    "    dim3 blockDim(8, 8);\n",
    "    dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n",
    "    sgemm<<<gridDim, blockDim>>>(dA, dB, dC, alpha, beta, width, height);\n",
    "    \n",
    "    // Copy computation result from the Device the host memory\n",
    "    cudaMemcpy(C_device.elements, dC.elements, width * height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    clock_gettime(CLOCK_MONOTONIC, &finish);\n",
    "    cudaEventRecord(stop, 0);\n",
    "    \n",
    "    // Estimate CUDA operation time\n",
    "    cudaEventRecord(stop, 0);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    cudaEventElapsedTime(&elapsed_gpu, start, stop);\n",
    "    printf(\"SGEMM CUDA Elapsed time: %f ms\\n\", elapsed_gpu);\n",
    "    elapsed_cpu = (finish.tv_sec - begin.tv_sec);\n",
    "    elapsed_cpu += (finish.tv_nsec - begin.tv_nsec) / 1000000000.0;\n",
    "    printf(\"Host time: %f ms\\n\", elapsed_cpu * 1000);\n",
    "    \n",
    "    // finalize CUDA event\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    // Finalize\n",
    "    cudaFree(dA.elements);\n",
    "    cudaFree(dB.elements);\n",
    "    cudaFree(dC.elements);\n",
    "    \n",
    "    free(A.elements);\n",
    "    free(B.elements);\n",
    "    //free(C_host.elements);\n",
    "    free(C_device.elements);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target) {\n",
    "    mat.width = width;\n",
    "    mat.height = height;\n",
    "    \n",
    "    if (target == DEVICE) {\n",
    "        cudaMalloc((void**)&mat.elements, width * height * sizeof(float));\n",
    "    }\n",
    "    else {\n",
    "        mat.elements = (float*)malloc(width * height * sizeof(float));\n",
    "    \n",
    "        for (int row = 0; row < height; row++) {\n",
    "            for (int col = 0; col < width; col++) {\n",
    "                mat.elements[row * width + col] = row * width + col * 0.001;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc sgemm_resize_block.cu --ptxas-options=--verbose -o sgemm_resize_block\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z5sgemm6MatrixS_S_ffii' for 'sm_20'\n",
      "ptxas info    : Function properties for _Z5sgemm6MatrixS_S_ffii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 10 registers, 96 bytes cmem[0]\n"
     ]
    }
   ],
   "source": [
    "! make sgemm_resize_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGEMM CUDA Elapsed time: 49.629311 ms\r\n",
      "Host time: 49.621832 ms\r\n"
     ]
    }
   ],
   "source": [
    "! ./sgemm_resize_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보시는 바와 같이 CUDA Occupancy에 따른 최적화 정도에 따라 CUDA 병렬화의 정도가 변하고, 성능과 밀접한 연관성이 있음을 확인 할 수 있었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
