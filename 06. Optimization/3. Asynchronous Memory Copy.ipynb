{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Asynchronous Memory Copy\n",
    "\n",
    "SGEMM 예제에서 측정한 수행시간을 보면 CPU / GPU 수행시간이 거의 비슷하다는 것을 볼 수 있습니다. 단순히 수행시간을 측정했고 속도가 빨라졌으니 좋은 일이지만, 따지고 보면 SGEMM 연산을 하는 동안 CPU는 아무것도 한 것이 없습니다. 즉, CUDA 연산이 끝날때까지 CPU는 CUDA의 동작이 끝나기를 기다린 것입니다. 이는 Memory를 복사하는 명령인 cudaMemcpy에서 CPU/GPU간 데이터 전송이 모두 끝나기 까지 다음 line으로 넘어가지 않기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sgemm_async_copy.cu\n"
     ]
    }
   ],
   "source": [
    "%%file sgemm_async_copy.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "typedef enum TARGET {HOST, DEVICE} TARGET;\n",
    "typedef enum MEMTYPE {NORMAL, PINNED} MEMTYPE;\n",
    "\n",
    "typedef struct {\n",
    "    int width;\n",
    "    int height;\n",
    "    float *elements;\n",
    "} Matrix;\n",
    "\n",
    "__global__ void sgemm(Matrix A, Matrix B, Matrix C, \n",
    "                      const float alpha, const float beta, \n",
    "                      const int width, const int height) {\n",
    "    int idx_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int idx_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int idx = idx_y * width + idx_x;\n",
    "    \n",
    "    if (idx_x >= width || idx_y >= height)\n",
    "        return;\n",
    "    \n",
    "    float value = 0.f;\n",
    "    for (int e = 0; e < width; e++)\n",
    "        value = alpha * A.elements[idx_y * width + e] * B.elements[e * width + idx_x];\n",
    "    C.elements[idx] = value + beta * C.elements[idx];\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target = HOST, MEMTYPE memtype = NORMAL);\n",
    "\n",
    "int main(int argv, char* argc[]) {\n",
    "    Matrix A, B, C;\n",
    "    Matrix dA, dB, dC;\n",
    "    const float alpha = 2.f;\n",
    "    const float beta = .5f;\n",
    "    const int width = 2048;\n",
    "    const int height = 2048;\n",
    "    float elapsed_gpu;\n",
    "    double elapsed_cpu;\n",
    "    \n",
    "    // Select Host memory type (NORMAL, PINNED)\n",
    "    MEMTYPE memtype = PINNED;\n",
    "    \n",
    "    // CUDA Event Create to estimate elased time\n",
    "    cudaEvent_t start, stop;\n",
    "    struct timespec begin, finish;\n",
    "    \n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Initialize host matrix\n",
    "    InitMatrix(A, width, height, HOST, memtype);\n",
    "    InitMatrix(B, width, height, HOST, memtype);\n",
    "    InitMatrix(C, width, height, HOST, memtype);\n",
    "\n",
    "    // CUDA Memory Initialize\n",
    "    InitMatrix(dA, width, height, DEVICE);\n",
    "    InitMatrix(dB, width, height, DEVICE);\n",
    "    InitMatrix(dC, width, height, DEVICE);\n",
    "    \n",
    "    // CUDA Operation\n",
    "    cudaEventRecord(start, 0);\n",
    "    clock_gettime(CLOCK_MONOTONIC, &begin);\n",
    "    \n",
    "    // Copy host data to the device (CUDA global memory)\n",
    "    // TODO: Write Asynchronous CUDA Memcpy API (gpu -> cpu)\n",
    "    \n",
    "    //////////////\n",
    "    \n",
    "    // Launch GPU Kernel\n",
    "    dim3 blockDim(16, 16);\n",
    "    dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n",
    "    sgemm<<<gridDim, blockDim>>>(dA, dB, dC, alpha, beta, width, height);\n",
    "    \n",
    "    // Copy computation result from the Device the host memory\n",
    "    // TODO: Write Asynchronous CUDA Memcpy API (cpu -> gpu)\n",
    "    \n",
    "    //////////////\n",
    "    clock_gettime(CLOCK_MONOTONIC, &finish);\n",
    "    cudaEventRecord(stop, 0);\n",
    "    \n",
    "    // Estimate CUDA operation time\n",
    "    cudaEventRecord(stop, 0);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    cudaEventElapsedTime(&elapsed_gpu, start, stop);\n",
    "    printf(\"SGEMM CUDA Elapsed time: %f ms\\n\", elapsed_gpu);\n",
    "    elapsed_cpu = (finish.tv_sec - begin.tv_sec);\n",
    "    elapsed_cpu += (finish.tv_nsec - begin.tv_nsec) / 1000000000.0;\n",
    "    printf(\"Host time: %f ms\\n\", elapsed_cpu * 1000);\n",
    "    \n",
    "    // finalize CUDA event\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    // Finalize\n",
    "    cudaFree(dA.elements);\n",
    "    cudaFree(dB.elements);\n",
    "    cudaFree(dC.elements);\n",
    "    \n",
    "    if (memtype == NORMAL) {\n",
    "        free(A.elements);\n",
    "        free(B.elements);\n",
    "        free(C.elements);\n",
    "    }\n",
    "    else {\n",
    "        // TODO: Write pinned memory free API\n",
    "\n",
    "        /////////////\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void InitMatrix(Matrix &mat, const int width, const int height, TARGET target, MEMTYPE memtype) {\n",
    "    mat.width = width;\n",
    "    mat.height = height;\n",
    "    \n",
    "    if (target == DEVICE) {\n",
    "        cudaMalloc((void**)&mat.elements, width * height * sizeof(float));\n",
    "    }\n",
    "    else {\n",
    "        if (memtype == NORMAL)\n",
    "            mat.elements = (float*)malloc(width * height * sizeof(float));\n",
    "        else\n",
    "            // TODO: Write pinned memory allocation API\n",
    "            \n",
    "            /////////////\n",
    "    \n",
    "        for (int row = 0; row < height; row++) {\n",
    "            for (int col = 0; col < width; col++) {\n",
    "                mat.elements[row * width + col] = row * width + col * 0.001;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc sgemm_async_copy.cu --ptxas-options=--verbose -gencode=arch=compute_35,code=sm_35 -I/usr/local/cuda/samples/common/inc -o sgemm_async_copy\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z5sgemm6MatrixS_S_ffii' for 'sm_35'\n",
      "ptxas info    : Function properties for _Z5sgemm6MatrixS_S_ffii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 13 registers, 384 bytes cmem[0]\n"
     ]
    }
   ],
   "source": [
    "! make sgemm_async_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGEMM CUDA Elapsed time: 30.752159 ms\r\n",
      "Host time: 0.038772 ms\r\n"
     ]
    }
   ],
   "source": [
    "! ./sgemm_async_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 처음 수행시간을 보면 비동기 동작은 확실히 일어나지 않았고, 처음과 같은 속도로 동작했습니다. 이는 비동기 동작을 하기 위해서는 Pinned Memory를 사용해야하기 때문입니다. 위 코드에서 malloc을 사용하던 코드에서 pinned memory를 사용할 수 있도록 수정해서 동작시간이 어떻게 변하는지 살펴보세요.\n",
    "\n",
    "2. GPU 동작시간은 비동기 동작을 했을 떄와 동일하게 바뀌었습니다. 한편 Host의 수행시간이 엄청나게 줄었습니다. 이는 곧 Host단에서 GPU 메모리 복사를 기다리지 않는 비동기 동작이 활성화 되었기 때문입니다. 하지만 이렇게 하는 경우에는 연산결과를 제대로 반영하지 않은채 함수나 프로그램이 종료될 수 있기 때문에 동기화를 하게 됩니다. CUDA는 다양한 방법을 제공하는데 우선은 GPU에 대하여 동기화를 하겠습니다. CUDA에서 GPU 동작에 대하여 Host의 동작을 동기화하는 명령은 아래와 같습니다.\n",
    "\n",
    "** cudaDeviceSynchronize() **\n",
    "\n",
    "위 코드에서 *cudaDeviceSynchronize()*의 위치가 Host 수행시간을 확인하는 코드와의 순서에 따라서 다르게 나타날 수 있으니 주의하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
