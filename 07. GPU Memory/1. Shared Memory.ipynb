{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Memory Optimization\n",
    "\n",
    "CUDA에서는 최적의 성능을 제공하기 위해서 다양한 메모리를 제공하고 있습니다.\n",
    "\n",
    "* L1/L2 Cache\n",
    "* Shared Memory\n",
    "* Constant Memory\n",
    "* Texture Memory\n",
    "* Global Memory\n",
    "\n",
    "이번 Tutorial에서는 L1/L2 Cache를 제외한 각각의 메모리가 갖는 특징을 예제를 통하여 살펴보도록 하겠습니다. 또한 Global Memory를 효율적으로 사용하기 위한 방법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.  Shared Memory\n",
    "\n",
    "Shared Memory는 데이터를 반복적으로 사용하는 데에 있어서 장점을 갖고 있습니다. SGEMM 예제를 통해서 이를 살펴보겠습니다.\n",
    "\n",
    "Shared Memory를 사용하는 방법은 다음과 같습니다.\n",
    "\n",
    "$ __shared__ <type> <variable name>[size]; $\n",
    "    \n",
    "이렇게 하면 CUDA block 내에서 Shared memory를 이용할 수 있게되며, CUDA Thread를 이용하여 shared memory에 데이터를 복사해 넣고 사용하면 됩니다.\n",
    "\n",
    "코드작성이 어려우시다면 [Solution](./sgemm_shared_solution.cu) 코드를 살펴보시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file sgemm_shared.cu\n",
    "\n",
    "#include \"sgemm.cuh\"\n",
    "template <typename T>\n",
    "__global__ void sgemm_shared(Matrix<T> A, Matrix<T> B, Matrix<T> C, \n",
    "                      const T alpha, const T beta, \n",
    "                      const int width, const int height) {\n",
    "    int idx_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int idx_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    \n",
    "    // TODO: Write shared memory declaration code\n",
    "    __shared__ T s_A;\n",
    "    __shared__ T s_B;\n",
    "    \n",
    "    // outer box move\n",
    "    T value = 0;\n",
    "    for (int step = 0; step < width; step += BLOCK_DIM) {\n",
    "        // TODO: Write block obtaining Code\n",
    "        s_A[threadIdx.y][threadIdx.x] = \n",
    "                            FIXME < width && FIXME < height ? \n",
    "                                A.elements[FIXME] : 0;\n",
    "        s_B[threadIdx.y][threadIdx.x] = \n",
    "                            FIXME < width && FIXME < height ? \n",
    "                                B.elements[FIXME] : 0;\n",
    "        \n",
    "        // Confirm that all required data is loaded\n",
    "        FIXME\n",
    "        \n",
    "        // inner operation\n",
    "        for (int e = 0; e < BLOCK_DIM; e++)\n",
    "            value += s_A[threadIdx.y][e] * s_B[e][threadIdx.x];\n",
    "    \n",
    "        // Confirm that all operation is finished above\n",
    "        FIXME\n",
    "    }\n",
    "\n",
    "    // Confirm that interested output only work\n",
    "    if (idx_x >= width || idx_y >= height)\n",
    "        return;\n",
    "    \n",
    "    // Write the result to device memory\n",
    "    C.elements[idx_y * width + idx_x] = alpha * value + beta * C.elements[idx_y * width + idx_x];\n",
    "}\n",
    "\n",
    "template <typename T>\n",
    "void launch_sgemm_shared(Matrix<T> &dA, Matrix<T> &dB, Matrix<T> &dC,\n",
    "                      const T alpha, const T beta, \n",
    "                      const int width, const int height) {\n",
    "    dim3 blockDim(16, 16);\n",
    "    dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n",
    "    sgemm_shared<<<gridDim, blockDim>>>(dA, dB, dC, alpha, beta, width, height);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file sgemm.cuh\n",
    "\n",
    "#ifndef _SGEMM_H_\n",
    "#define _SGEMM_H_\n",
    "\n",
    "#define BLOCK_DIM 16\n",
    "\n",
    "typedef enum TARGET {HOST, DEVICE} TARGET;\n",
    "typedef enum MEMTYPE {NORMAL, PINNED} MEMTYPE;\n",
    "\n",
    "template <typename T>\n",
    "struct Matrix {\n",
    "    int width;\n",
    "    int height;\n",
    "    T *elements;\n",
    "};\n",
    "\n",
    "#endif /* _SGEMM_H_ */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file sgemm.cu\n",
    "\n",
    "#include \"sgemm.cuh\"\n",
    "template <typename T>\n",
    "__global__ void sgemm(Matrix<T> A, Matrix<T> B, Matrix<T> C, \n",
    "                      const T alpha, const T beta, \n",
    "                      const int width, const int height) {\n",
    "    int idx_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int idx_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    \n",
    "    if (idx_x >= width || idx_y >= height)\n",
    "        return;\n",
    "    \n",
    "    T value = 0;\n",
    "    for (int e = 0; e < width; e++)\n",
    "        value += A.elements[idx_y * width + e] * B.elements[e * width + idx_x];\n",
    "    C.elements[idx_y * width + idx_x] = alpha * value + beta * C.elements[idx_y * width + idx_x];\n",
    "}\n",
    "\n",
    "template <typename T>\n",
    "void launch_sgemm(Matrix<T> &dA, Matrix<T> &dB, Matrix<T> &dC,\n",
    "                      const T alpha, const T beta, \n",
    "                      const int width, const int height) {\n",
    "    dim3 blockDim(16, 16);\n",
    "    dim3 gridDim((width + blockDim.x - 1) / blockDim.x, (height + blockDim.y - 1) / blockDim.y);\n",
    "    sgemm<<<gridDim, blockDim>>>(dA, dB, dC, alpha, beta, width, height);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_shared.cu\n"
     ]
    }
   ],
   "source": [
    "%%file test_shared.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"sgemm.cuh\"\n",
    "#include \"sgemm.cu\"\n",
    "#include \"sgemm_shared_solution.cu\"\n",
    "\n",
    "template <typename T>\n",
    "void InitMatrix(Matrix<T> &mat, const int width, const int height, TARGET target = HOST, MEMTYPE memtype = NORMAL);\n",
    "\n",
    "template <typename T>\n",
    "bool IsMatDiff(Matrix<T> &A, Matrix<T> &B);\n",
    "\n",
    "int main(int argv, char* argc[]) {\n",
    "    Matrix<float> A, B, C, D;\n",
    "    Matrix<float> dA, dB, dC, dD;\n",
    "    const float alpha = 2.f;\n",
    "    const float beta = .5f;\n",
    "    const int width = 4;\n",
    "    const int height = width;\n",
    "    float elapsed_gpu;\n",
    "    double elapsed_cpu;\n",
    "    \n",
    "    // Select Host memory type (NORMAL, PINNED)\n",
    "    MEMTYPE memtype = PINNED;\n",
    "    \n",
    "    // CUDA Event Create to estimate elased time\n",
    "    cudaEvent_t start_org, stop_org, start_opt, stop_opt;\n",
    "    struct timespec begin, finish;\n",
    "    \n",
    "    cudaEventCreate(&start_org);\n",
    "    cudaEventCreate(&stop_org);\n",
    "    cudaEventCreate(&start_opt);\n",
    "    cudaEventCreate(&stop_opt);\n",
    "    \n",
    "    // Initialize host matrix\n",
    "    InitMatrix(A, width, height, HOST, memtype);\n",
    "    InitMatrix(B, width, height, HOST, memtype);\n",
    "    InitMatrix(C, width, height, HOST, memtype);\n",
    "    InitMatrix(D, width, height, HOST, memtype);\n",
    "\n",
    "    // CUDA Memory Initialize\n",
    "    InitMatrix(dA, width, height, DEVICE);\n",
    "    InitMatrix(dB, width, height, DEVICE);\n",
    "    InitMatrix(dC, width, height, DEVICE);\n",
    "    InitMatrix(dD, width, height, DEVICE);\n",
    "    \n",
    "    // CUDA Operation\n",
    "    clock_gettime(CLOCK_MONOTONIC, &begin);\n",
    "    \n",
    "    // Copy host data to the device (CUDA global memory)\n",
    "    cudaMemcpyAsync(dA.elements, A.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpyAsync(dB.elements, B.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpyAsync(dC.elements, C.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpyAsync(dD.elements, D.elements, width * height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch GPU Kernel\n",
    "    cudaEventRecord(start_org, 0);\n",
    "    launch_sgemm(dA, dB, dC, alpha, beta, width, height);\n",
    "    cudaEventRecord(stop_org, 0);\n",
    "    cudaEventSynchronize(stop_org);\n",
    "    \n",
    "    cudaEventRecord(start_opt, 0);\n",
    "    launch_sgemm_shared(dA, dB, dD, alpha, beta, width, height);\n",
    "    cudaEventRecord(stop_opt, 0);\n",
    "    cudaEventSynchronize(stop_opt);\n",
    "    \n",
    "    // Copy computation result from the Device the host memory\n",
    "    cudaMemcpyAsync(C.elements, dC.elements, width * height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpyAsync(D.elements, dD.elements, width * height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Estimate CUDA operation time\n",
    "    cudaDeviceSynchronize();\n",
    "    clock_gettime(CLOCK_MONOTONIC, &finish);\n",
    "    \n",
    "    cudaEventElapsedTime(&elapsed_gpu, start_org, stop_org);\n",
    "    printf(\"SGEMM CUDA Elapsed time (original): %f ms\\n\", elapsed_gpu);\n",
    "    cudaEventElapsedTime(&elapsed_gpu, start_opt, stop_opt);\n",
    "    printf(\"SGEMM CUDA Elapsed time (shared): %f ms\\n\", elapsed_gpu);\n",
    "    elapsed_cpu = (finish.tv_sec - begin.tv_sec);\n",
    "    elapsed_cpu += (finish.tv_nsec - begin.tv_nsec) / 1000000000.0;\n",
    "    printf(\"Host time: %f ms\\n\", elapsed_cpu * 1000);\n",
    "    \n",
    "    if (IsMatDiff(C, D)) {\n",
    "        printf(\"Something wrong!!\\n\");\n",
    "    }\n",
    "    else {\n",
    "        printf(\"Success !!\\n\");\n",
    "    }\n",
    "    \n",
    "    // finalize CUDA event\n",
    "    cudaEventDestroy(start_org);\n",
    "    cudaEventDestroy(stop_org);\n",
    "    cudaEventDestroy(start_opt);\n",
    "    cudaEventDestroy(stop_opt);\n",
    "    \n",
    "    // Finalize\n",
    "    cudaFree(dA.elements);\n",
    "    cudaFree(dB.elements);\n",
    "    cudaFree(dC.elements);\n",
    "    cudaFree(dD.elements);\n",
    "    \n",
    "    if (memtype == NORMAL) {\n",
    "        free(A.elements);\n",
    "        free(B.elements);\n",
    "        free(C.elements);\n",
    "        free(D.elements);\n",
    "    }\n",
    "    else {\n",
    "        cudaFreeHost(A.elements);\n",
    "        cudaFreeHost(B.elements);\n",
    "        cudaFreeHost(C.elements);\n",
    "        cudaFreeHost(D.elements);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\n",
    "template <typename T>\n",
    "void InitMatrix(Matrix<T> &mat, const int width, const int height, TARGET target, MEMTYPE memtype) {\n",
    "    mat.width = width;\n",
    "    mat.height = height;\n",
    "    \n",
    "    if (target == DEVICE) {\n",
    "        cudaMalloc((void**)&mat.elements, width * height * sizeof(T));\n",
    "    }\n",
    "    else {\n",
    "        if (memtype == NORMAL)\n",
    "            mat.elements = (T*)malloc(width * height * sizeof(T));\n",
    "        else\n",
    "            cudaHostAlloc(&mat.elements, width * height * sizeof(T), cudaHostAllocDefault);\n",
    "        \n",
    "        for (int row = 0; row < height; row++) {\n",
    "            for (int col = 0; col < width; col++) {\n",
    "                mat.elements[row * width + col] = 1.f;//row * width + col * 0.001;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "template <typename T>\n",
    "bool IsMatDiff(Matrix<T> &A, Matrix<T> &B) {\n",
    "    if (A.width != B.width || A.height != B.height) {\n",
    "        return true;\n",
    "    }\n",
    "    \n",
    "    int count = 0;\n",
    "    for (int row = 0; row < A.height; row++) {\n",
    "        for (int col = 0; col < A.width; col++) {\n",
    "            count |= (A.elements[row * A.width + col] != B.elements[row * A.width + col]) ? 0x1 : 0x0;\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for (int row = 0; row < A.height; row++) {\n",
    "        for (int col = 0; col < A.width; col++) {\n",
    "            printf(\"%f \", A.elements[row * A.width + col]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    for (int row = 0; row < B.height; row++) {\n",
    "        for (int col = 0; col < B.width; col++) {\n",
    "            printf(\"%f \", B.elements[row * B.width + col]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    } \n",
    "    \n",
    "    if (count != 0) {\n",
    "        printf(\"Count: %d\\n\", count);\n",
    "        return true;\n",
    "    }\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc --ptxas-options=--verbose -gencode arch=compute_30,code=sm_30 -I/usr/local/cuda/samples/common/inc test_shared.cu -c test_shared.o\n",
      "ptxas info    : 0 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z12sgemm_sharedIfEv6MatrixIT_ES2_S2_S1_S1_ii' for 'sm_30'\n",
      "ptxas info    : Function properties for _Z12sgemm_sharedIfEv6MatrixIT_ES2_S2_S1_S1_ii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 23 registers, 2048 bytes smem, 384 bytes cmem[0]\n",
      "ptxas info    : Compiling entry function '_Z5sgemmIfEv6MatrixIT_ES2_S2_S1_S1_ii' for 'sm_30'\n",
      "ptxas info    : Function properties for _Z5sgemmIfEv6MatrixIT_ES2_S2_S1_S1_ii\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 17 registers, 384 bytes cmem[0]\n",
      "nvcc --ptxas-options=--verbose -gencode arch=compute_30,code=sm_30 -I/usr/local/cuda/samples/common/inc test_shared.o -o test_shared\n"
     ]
    }
   ],
   "source": [
    "! make test_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGEMM CUDA Elapsed time (original): 0.019072 ms\r\n",
      "SGEMM CUDA Elapsed time (shared): 0.012000 ms\r\n",
      "Host time: 0.092196 ms\r\n",
      "8.500000 8.500000 8.500000 8.500000 \r\n",
      "8.500000 8.500000 8.500000 8.500000 \r\n",
      "8.500000 8.500000 8.500000 8.500000 \r\n",
      "8.500000 8.500000 8.500000 8.500000 \r\n",
      "8.500000 8.500000 8.500000 8.500000 \r\n",
      "8.500000 8.500000 8.500000 8.500000 \r\n",
      "8.500000 8.500000 8.500000 8.500000 \r\n",
      "8.500000 8.500000 8.500000 8.500000 \r\n",
      "Success !!\r\n"
     ]
    }
   ],
   "source": [
    "! ./test_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 예제에서는 Kernel의 수행 시간만을 측정했습니다.\n",
    "\n",
    "수행 결과, Shared Memory를 사용하기 전과 후를 비교했을 때, CUDA Kernel의 성능 향상이 있었음을 확인할 수 있었습니다.\n",
    "\n",
    "** 연습해보기 **\n",
    "1. sgemm_shared CUDA Kernel에서 사용한 **__syncthread()**를 사용하기 전과 후를 비교해 보세요.\n",
    "1. Matrix의 크기를 바꾸어가면서 해보세요. 예제 코드에서는 Squared Matrix만이 되도록 되어있습니다만, 수정하시면 다른 형태로도 사용할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
