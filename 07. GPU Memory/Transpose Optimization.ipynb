{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 Transpose 동작에 대하여 최적화를 하는 실습을 해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naïve Transpose\n",
    "행렬에서 Transpose 동작은 행렬의 요소들을 이항시키는 작업을 의미합니다.\n",
    "\n",
    "\"01. CUDA Basic\"에서 Transpose 동작에 대하여 실습을 해 보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1.cu\n"
     ]
    }
   ],
   "source": [
    "%%file task1.cu\n",
    "#include <stdio.h>\n",
    "#include \"util.c\"\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "#define INDEX( row, col, width ) ( ( (row) * (width) ) + (col) )\n",
    "\n",
    "__global__ \n",
    "void d_transpose(float *d_out, float *d_in, int n_width, int n_height) {\n",
    "    int row = FIXME\n",
    "    int col = FIXME\n",
    "    \n",
    "    // TODO: Write transpose code\n",
    "    d_out[FIXME] = d_in[FIXME];\n",
    "}\n",
    "\n",
    "void transpose(float *p_out, float *p_in, int n_width, int n_height) {\n",
    "    for (int j = 0; j < n_height; j++) {\n",
    "        for (int i = 0; i < n_width; i++) {\n",
    "            p_out[INDEX(i, j, n_height)] = p_in[INDEX(j, i, n_width)];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float *p_in, *p_out, *p_out_cuda;\n",
    "    float *d_in, *d_out;\n",
    "    \n",
    "    int n_width = 4096;\n",
    "    int n_height = 3072;\n",
    "    \n",
    "    p_in = get_buffer(n_width * n_height);\n",
    "    p_out = get_buffer(n_width * n_height);\n",
    "    p_out_cuda = get_buffer(n_width * n_height);\n",
    "    \n",
    "    /* create and start timer */\n",
    "\n",
    "    cudaEvent_t start, stop;\n",
    "    float elapsedTime;\n",
    "    cudaEventCreate( &start );\n",
    "    cudaEventCreate( &stop );\n",
    "    \n",
    "    // Step 1. Allocate to GPU memory\n",
    "    cudaMalloc((void**)&d_in, n_width * n_height * sizeof(float));\n",
    "    cudaMalloc((void**)&d_out, n_width * n_height * sizeof(float));\n",
    "    \n",
    "    // Initialize input data\n",
    "    for (int j = 0; j < n_height; j++) {\n",
    "        for (int i = 0; i < n_width; i++) {\n",
    "            p_in[j * n_width + i] = float(j * n_width + i);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Step 2. Copy to GPU memory\n",
    "    cudaMemcpy(d_in, p_in, n_width * n_height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaEventRecord( start, 0 );\n",
    "    printf(\"%s[%d]\\n\", __FILE__, __LINE__);\n",
    "    // CPU transpose\n",
    "    transpose(p_out, p_in, n_width, n_height);\n",
    "    printf(\"%s[%d]\\n\", __FILE__, __LINE__);\n",
    "    \n",
    "    cudaEventRecord( stop, 0 );\n",
    "    cudaEventSynchronize( stop );\n",
    "    cudaEventElapsedTime( &elapsedTime, start, stop );\n",
    "    \n",
    "    // print CPU performance\n",
    "    fprintf(stdout, \"Total time CPU is %f sec\\n\", elapsedTime / 1000.0f );\n",
    "    fprintf(stdout, \"Performance is %f GB/s\\n\", 8.0 * 2.0 * (double) n_width * (double) n_height / \n",
    "        ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\n",
    "    \n",
    "    // Step 3. Kernel leaunch\n",
    "    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 gridDim(n_width / BLOCK_SIZE + 1, n_height / BLOCK_SIZE + 1);\n",
    "    \n",
    "    cudaEventRecord( start, 0 );\n",
    "    \n",
    "    d_transpose<<<gridDim, blockDim>>>(d_out, d_in, n_width, n_height);\n",
    "    \n",
    "    cudaEventRecord( stop, 0 );\n",
    "    cudaEventSynchronize( stop );\n",
    "    cudaEventElapsedTime( &elapsedTime, start, stop );\n",
    "    \n",
    "    // print GPU performance\n",
    "    fprintf(stdout, \"Total time GPU is %f sec\\n\", elapsedTime / 1000.0f );\n",
    "    fprintf(stdout, \"Performance is %f GB/s\\n\", 8.0 * 2.0 * (double) n_width * (double) n_height / \n",
    "        ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\n",
    "    \n",
    "    // Step 4. Copy from GPU\n",
    "    cudaMemcpy(p_out_cuda, d_out, n_width * n_height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Step 5. check result\n",
    "    check_result(p_out, p_out_cuda, n_width * n_height);\n",
    "    \n",
    "    // Step 6. free GPU memory\n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    \n",
    "    free(p_in);\n",
    "    free(p_out);\n",
    "    free(p_out_cuda);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1.cu(9): error: identifier \"FIXME\" is undefined\r\n",
      "\r\n",
      "task1.cu(10): error: expected a \";\"\r\n",
      "\r\n",
      "task1.cu(13): warning: parsing restarts here after previous syntax error\r\n",
      "\r\n",
      "2 errors detected in the compilation of \"/tmp/tmpxft_00000082_00000000-8_task1.cpp1.ii\".\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -lineinfo -DDEBUG -arch=sm_35 -o task1.out task1.cu && echo Compiled Successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1.cu[64]\r\n",
      "task1.cu[67]\r\n",
      "Total time CPU is 0.021915 sec\r\n",
      "Performance is 1.513950 GB/s\r\n",
      "Total time CPU is 0.000172 sec\r\n",
      "Performance is 193.324628 GB/s\r\n",
      "Result: 0\r\n"
     ]
    }
   ],
   "source": [
    "!./task1.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Shared Memory as Transpose buffer\n",
    "GPU의 Global memory를 보다 효율적으로 활용하기 위해서는 Shared Memory를 사용하면 된다는 것을 배웠습니다.\n",
    "Transpose에서 Shared Memory의 효과를 확인해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file task2.cu\n",
    "#include <stdio.h>\n",
    "#include \"util.c\"\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "#define INDEX( row, col, width ) ( ( (row) * (width) ) + (col) )\n",
    "\n",
    "__global__ \n",
    "void d_transpose(float *d_out, float *d_in, int n_width, int n_height) {\n",
    "    int col = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int row = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    \n",
    "    __shared__ double s_buffer[BLOCK_SIZE][BLOCK_SIZE];\n",
    "    \n",
    "    int tileX = blockDim.x * blockIdx.x;\n",
    "    int tileY = blockDim.y * blockIdx.y;\n",
    "    \n",
    "    if (col < n_width && row < n_height)\n",
    "        s_buffer[threadIdx.y][threadIdx.x] = d_in[INDEX(tileY + threadIdx.y, tileX + threadIdx.x, n_width)];\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (col < n_width && row < n_height)\n",
    "        d_out[INDEX(tileX + threadIdx.y, tileY + threadIdx.x, n_height)] = s_buffer[threadIdx.x][threadIdx.y];\n",
    "}\n",
    "\n",
    "void transpose(float *p_out, float *p_in, int n_width, int n_height) {\n",
    "    for (int j = 0; j < n_height; j++) {\n",
    "        for (int i = 0; i < n_width; i++) {\n",
    "            p_out[INDEX(i, j, n_height)] = p_in[INDEX(j, i, n_width)];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float *p_in, *p_out, *p_out_cuda;\n",
    "    float *d_in, *d_out;\n",
    "    \n",
    "    int n_width = 4096;\n",
    "    int n_height = 3072;\n",
    "    \n",
    "    p_in = get_buffer(n_width * n_height);\n",
    "    p_out = get_buffer(n_width * n_height);\n",
    "    p_out_cuda = get_buffer(n_width * n_height);\n",
    "    \n",
    "    /* create and start timer */\n",
    "\n",
    "    cudaEvent_t start, stop;\n",
    "    float elapsedTime;\n",
    "    cudaEventCreate( &start );\n",
    "    cudaEventCreate( &stop );\n",
    "    \n",
    "    // Step 1. Allocate to GPU memory\n",
    "    cudaMalloc((void**)&d_in, n_width * n_height * sizeof(float));\n",
    "    cudaMalloc((void**)&d_out, n_width * n_height * sizeof(float));\n",
    "    \n",
    "    // Initialize input data\n",
    "    for (int j = 0; j < n_height; j++) {\n",
    "        for (int i = 0; i < n_width; i++) {\n",
    "            p_in[j * n_width + i] = float(j * n_width);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Step 2. Copy to GPU memory\n",
    "    cudaMemcpy(d_in, p_in, n_width * n_height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaEventRecord( start, 0 );\n",
    "    \n",
    "    // CPU transpose\n",
    "    transpose(p_out, p_in, n_width, n_height);\n",
    "    \n",
    "    cudaEventRecord( stop, 0 );\n",
    "    cudaEventSynchronize( stop );\n",
    "    cudaEventElapsedTime( &elapsedTime, start, stop );\n",
    "    \n",
    "    // print CPU performance\n",
    "    fprintf(stdout, \"Total time CPU is %f sec\\n\", elapsedTime / 1000.0f );\n",
    "    fprintf(stdout, \"Performance is %f GB/s\\n\", 8.0 * 2.0 * (double) n_width * (double) n_height / \n",
    "        ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\n",
    "    \n",
    "    // Step 3. Kernel leaunch\n",
    "    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 gridDim(n_width / BLOCK_SIZE + 1, n_height / BLOCK_SIZE + 1);\n",
    "    \n",
    "    cudaEventRecord( start, 0 );\n",
    "    \n",
    "    d_transpose<<<gridDim, blockDim>>>(d_out, d_in, n_width, n_height);\n",
    "    \n",
    "    cudaEventRecord( stop, 0 );\n",
    "    cudaEventSynchronize( stop );\n",
    "    cudaEventElapsedTime( &elapsedTime, start, stop );\n",
    "    \n",
    "    // print GPU performance\n",
    "    fprintf(stdout, \"Total time GPU is %f sec\\n\", elapsedTime / 1000.0f );\n",
    "    fprintf(stdout, \"Performance is %f GB/s\\n\", 8.0 * 2.0 * (double) n_width * (double) n_height / \n",
    "        ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\n",
    "    \n",
    "    // Step 4. Copy from GPU\n",
    "    cudaMemcpy(p_out_cuda, d_out, n_width * n_height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Step 5. check result\n",
    "    check_result(p_out, p_out_cuda, n_width * n_height);\n",
    "    \n",
    "    // Step 6. free GPU memory\n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    \n",
    "    free(p_in);\n",
    "    free(p_out);\n",
    "    free(p_out_cuda);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled Successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -lineinfo -DDEBUG -arch=sm_35 -o task2.out task2.cu && echo Compiled Successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time CPU is 0.346841 sec\n",
      "Performance is 0.580459 GB/s\n",
      "Total time GPU is 0.000341 sec\n",
      "Performance is 589.750270 GB/s\n",
      "Result: 0\n"
     ]
    }
   ],
   "source": [
    "!./task2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. No-Bank Conflict\n",
    "위 예제를 통해 GPU에 따라 이전과 비교하여 성능에 변화가 생겼습니다.\n",
    "사용하는 GPU에 따라 성능은 이전보다 빠를 수도 느릴 수도 있습니다. 하지만 어떤 경우라도 사실 이것은 최적의 결과는 아닙니다.\n",
    "이유는 이전의 경우 Transpose를 하는 과정에서 Shared memory를 사용하는데 bank conflict를 유발하고 있기 때문입니다.\n",
    "\n",
    "Bank Conflict는 Share memory의 경우 여느 메모리와 마찬가지로 bank로 관리가 됩니다. 만일 복수의 CUDA thread가 동시에 하나의 메모리 bank에 있는 주소에 접근을 하려 하는 경우, memory bank는 순차적으로 처리를 하게 되기에 처리속도에 영향을 주게 됩니다. 이를 피하기 위해서는 shared memory에 접근하는 pattern을 개선해야 할 필요가 있습니다.\n",
    "\n",
    "가장 간단한 방법으로는 가로 방향으로 1칸의 크기를 더 할당하는 것입니다.\n",
    "Transpose를 할 때, 처음 shared memory에 데이터를 복사할 때는 각기 다른 bank를 사용하여 bank conflict가 발생하지 않습니다. 하지만 transpose된 형태로 데이터를 읽어 들일때 하나의 bank에 집중되는 현상이 나타나므로, shared memory에 공간을 비틀어서 bank conflict를 우회하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file task3.cu\n",
    "#include <stdio.h>\n",
    "#include \"util.c\"\n",
    "\n",
    "#define BLOCK_SIZE 16\n",
    "#define INDEX( row, col, width ) ( ( (row) * (width) ) + (col) )\n",
    "\n",
    "__global__ \n",
    "void d_transpose(float *d_out, float *d_in, int n_width, int n_height) {\n",
    "    int col = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int row = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    \n",
    "    __shared__ double s_buffer[BLOCK_SIZE][BLOCK_SIZE+1];\n",
    "    \n",
    "    int tileX = blockDim.x * blockIdx.x;\n",
    "    int tileY = blockDim.y * blockIdx.y;\n",
    "    \n",
    "    if (col < n_width && row < n_height)\n",
    "        s_buffer[threadIdx.y][threadIdx.x] = d_in[INDEX(tileY + threadIdx.y, tileX + threadIdx.x, n_width)];\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (col < n_width && row < n_height)\n",
    "        d_out[INDEX(tileX + threadIdx.y, tileY + threadIdx.x, n_height)] = s_buffer[threadIdx.x][threadIdx.y];\n",
    "}\n",
    "\n",
    "void transpose(float *p_out, float *p_in, int n_width, int n_height) {\n",
    "    for (int j = 0; j < n_height; j++) {\n",
    "        for (int i = 0; i < n_width; i++) {\n",
    "            p_out[INDEX(i, j, n_height)] = p_in[INDEX(j, i, n_width)];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float *p_in, *p_out, *p_out_cuda;\n",
    "    float *d_in, *d_out;\n",
    "    \n",
    "    int n_width = 4096;\n",
    "    int n_height = 3072;\n",
    "    \n",
    "    p_in = get_buffer(n_width * n_height);\n",
    "    p_out = get_buffer(n_width * n_height);\n",
    "    p_out_cuda = get_buffer(n_width * n_height);\n",
    "    \n",
    "    /* create and start timer */\n",
    "\n",
    "    cudaEvent_t start, stop;\n",
    "    float elapsedTime;\n",
    "    cudaEventCreate( &start );\n",
    "    cudaEventCreate( &stop );\n",
    "    \n",
    "    // Step 1. Allocate to GPU memory\n",
    "    cudaMalloc((void**)&d_in, n_width * n_height * sizeof(float));\n",
    "    cudaMalloc((void**)&d_out, n_width * n_height * sizeof(float));\n",
    "    \n",
    "    // Initialize input data\n",
    "    for (int j = 0; j < n_height; j++) {\n",
    "        for (int i = 0; i < n_width; i++) {\n",
    "            p_in[j * n_width + i] = float(j * n_width);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Step 2. Copy to GPU memory\n",
    "    cudaMemcpy(d_in, p_in, n_width * n_height * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaEventRecord( start, 0 );\n",
    "    \n",
    "    // CPU transpose\n",
    "    transpose(p_out, p_in, n_width, n_height);\n",
    "    \n",
    "    cudaEventRecord( stop, 0 );\n",
    "    cudaEventSynchronize( stop );\n",
    "    cudaEventElapsedTime( &elapsedTime, start, stop );\n",
    "    \n",
    "    // print CPU performance\n",
    "    fprintf(stdout, \"Total time CPU is %f sec\\n\", elapsedTime / 1000.0f );\n",
    "    fprintf(stdout, \"Performance is %f GB/s\\n\", 8.0 * 2.0 * (double) n_width * (double) n_height / \n",
    "        ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\n",
    "    \n",
    "    // Step 3. Kernel leaunch\n",
    "    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 gridDim(n_width / BLOCK_SIZE + 1, n_height / BLOCK_SIZE + 1);\n",
    "    \n",
    "    cudaEventRecord( start, 0 );\n",
    "    \n",
    "    d_transpose<<<gridDim, blockDim>>>(d_out, d_in, n_width, n_height);\n",
    "    \n",
    "    cudaEventRecord( stop, 0 );\n",
    "    cudaEventSynchronize( stop );\n",
    "    cudaEventElapsedTime( &elapsedTime, start, stop );\n",
    "    \n",
    "    // print GPU performance\n",
    "    fprintf(stdout, \"Total time GPU is %f sec\\n\", elapsedTime / 1000.0f );\n",
    "    fprintf(stdout, \"Performance is %f GB/s\\n\", 8.0 * 2.0 * (double) n_width * (double) n_height / \n",
    "        ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\n",
    "    \n",
    "    // Step 4. Copy from GPU\n",
    "    cudaMemcpy(p_out_cuda, d_out, n_width * n_height * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Step 5. check result\n",
    "    check_result(p_out, p_out_cuda, n_width * n_height);\n",
    "    \n",
    "    // Step 6. free GPU memory\n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    \n",
    "    free(p_in);\n",
    "    free(p_out);\n",
    "    free(p_out_cuda);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled Successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -lineinfo -DDEBUG -arch=sm_35 -o task3.out task3.cu && echo Compiled Successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time CPU is 0.318297 sec\n",
      "Performance is 0.632512 GB/s\n",
      "Total time GPU is 0.000270 sec\n",
      "Performance is 745.256614 GB/s\n",
      "Result: 0\n"
     ]
    }
   ],
   "source": [
    "!./task3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
